{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c171c391-ca9c-4591-8659-a13cef0a6d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.15.0\n",
      "Pytorch version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Pytorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1925509-70af-4726-9455-5fb1e6063a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6506851500204207a754f20619a06cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8aead528ebb4678ad5cf43fe0281d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baea603f7bd43e0ae97a3f6310a9301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664cb2bcfb9f434db0f5126cf3cb4210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa0424998a947c785ccaa35b704f7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e59c07b61f745f9b74777fa22e1ef87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what's your name ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_GPT: I'm not sure, but I think it's a reference to the movie The Big Lebowski.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  quit\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Disable warnings about padding_side that cannot be rectified with current software:\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model_names = [\"microsoft/DialoGPT-small\", \"microsoft/DialoGPT-medium\", \"microsoft/DialoGPT-large\"]\n",
    "use_model_index = 1  # Change 0: small model, 1: medium, 2: large model (requires most resources!)\n",
    "model_name = model_names[use_model_index]\n",
    "          \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # , padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# The chat function: received a user input and chat-history and returns the model's reply and chat-history:\n",
    "def reply(input_text, history=None):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([history, new_user_input_ids], dim=-1) if history is not None else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True), chat_history_ids\n",
    "\n",
    "history = None\n",
    "while True:\n",
    "    input_text = input(\"> \")\n",
    "    if input_text in [\"\", \"bye\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "    reply_text, history_new = reply(input_text, history)\n",
    "    history=history_new\n",
    "    if history.shape[1]>80:\n",
    "        old_shape = history.shape\n",
    "        history = history[:,-80:]\n",
    "        print(f\"History cut from {old_shape} to {history.shape}\")\n",
    "    # history_text = tokenizer.decode(history[0])\n",
    "    # print(f\"Current history: {history_text}\")\n",
    "    print(f\"D_GPT: {reply_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
